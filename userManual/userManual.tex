%% LyX 2.3.4.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english,format=acmsmall]{article}
\usepackage{newcent}
\usepackage[scaled=0.85]{beramono}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm,headheight=2.5cm,headsep=2.5cm,footskip=1cm}
\usepackage{babel}
\usepackage{array}
\usepackage{url}
\usepackage{amstext}
\usepackage{rotating}
\usepackage{xargs}[2008/03/08]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand*\LyXZeroWidthSpace{\hspace{0pt}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% recommended but missing
%\usepackage{orcidlink}
%\usepackage{thumbpdf}


 
\usepackage{amssymb} % Provides \checkmark

\makeatother

\usepackage{listings}
\lstset{basicstyle={\ttfamily\small},
breaklines=false,
columns=flexible,
keepspaces=true,
moredelim={**[is][\bfseries]{~}{~}}}
\renewcommand{\lstlistingname}{Listing}

\begin{document}
\title{User's Guide for UltimateKalman:\\
a Library for Flexible Kalman Filtering and Smoothing Using Orthogonal
Transformations}
\author{Sivan Toledo}

\maketitle
\global\long\def\linearestimator{F}%

\global\long\def\modelfun{M}%

\global\long\def\penaltyfun{\phi}%

\global\long\def\objectivefun{\phi}%

\global\long\def\grad{\nabla}%

\newcommandx\hessian[1][usedefault, addprefix=\global, 1=]{\nabla_{#1}^{2}}%

\global\long\def\jacobian{\mathrm{J}}%

\global\long\def\exact#1{#1}%

\global\long\def\estimate#1{\hat{#1}}%

\global\long\def\controlpoint{\rho}%

\global\long\def\location{\ell}%

\global\long\def\noise{\epsilon}%

\global\long\def\observations{b}%

\global\long\def\expectation{\operatorname{E}}%

\global\long\def\iu{\mathbf{i}}%

\global\long\def\vecone{\mathbf{1}}%

\global\long\def\veczero{\mathbf{0}}%

\global\long\def\covold{\text{cov}}%

\global\long\def\nonjsscov{\operatorname{cov}}%

\global\long\def\cov{\operatorname{cov}}%

\global\long\def\var{\text{var}}%

\global\long\def\fim{\mathcal{I}}%

\global\long\def\loglikelihood{\mathcal{L}}%

\global\long\def\score{\mathcal{S}}%

\global\long\def\duration{\vartheta}%

\global\long\def\attenuation{a}%
\global\long\def\cmplxatt{\alpha}%

\global\long\def\initialphase{\varphi}%

\global\long\def\satclockerr{\eta}%

\global\long\def\ionodelay{\psi}%

\global\long\def\tropodelay{\xi}%

\global\long\def\xcorr{\operatorname{xcorr}}%

\global\long\def\diag{\operatorname{diag}}%

\global\long\def\rank{\operatorname{rank}}%

\global\long\def\erf{\operatorname{erf}}%

\global\long\def\erfc{\operatorname{erfc}}%

\global\long\def\range{\operatorname{range}}%

\global\long\def\trace{\operatorname{trace}}%

\global\long\def\ops{\operatorname{ops}}%

\global\long\def\prob{\operatorname{Prob}}%

\global\long\def\real{\text{Re}}%

\global\long\def\imag{\text{Im}}%

\global\long\def\square{\text{\ensuremath{\blacksquare}}}%

\global\long\def\irange{\boldsymbol{:}}%

UltimateKalman is a library that implements efficient and flexible
Kalman filtering and smoothing algorithms, including parallel (multi-core)
smoothers. The library contains MATLAB, C, and Java implementations
(currently, the Java implementations does not contain all the algorithms).
The library is robust: it includes mechanisms for testing and evaluating
the performance of all the implementations, as well as with a set
of well-documented examples. The library is available on GitHub at
\url{https://github.com/sivantoledo/ultimate-kalman}. The release
history of the library is as follows:
\begin{itemize}
\item \textbf{Release 1.2.0} contains the sequential UltimateKalman algorithm,
which is documented carefully in an article in the ACM Transactions
on Mathematical Software~\cite{doi:10.1145/3699958}. The algorithm
is a slight extension an algorithm by Paige and Saunders~\cite{PaigeSaunders:1977:Kalman},
which to the best of our knowledge, has not been implemented before.
The algorithm uses orthogonal transformations so it has good numerical
stability. The algorithm is implemented monolithically in all 3 languages.
Please cite this article when citing the sequential UltimateKalman
algorithm or its implementation. To use that version, please use the
user guide from that release, not this document.
\item \textbf{Release 2.0.0} contains three additional algorithms in MATLAB
and C, including two parallel-in-time smoothers, the Odd-Even smoother
proposed in an article by Gargir and Toledo~\cite{toappear:IPDPS2025}
and the smoother proposed by Särkkä and García-Fernández~\cite{10.1109/TAC.2020.2976316}.
The code in this release was used to perform the experiments reported
by Gargir and Toledo~\cite{toappear:IPDPS2025}; the release is meant
mostly to document these experiments, not for adoption by users.
\item \textbf{Release 2.1.0} is a significantly cleaned up version of the
codes described by Gargir and Toledo~\cite{toappear:IPDPS2025}.
It is meant to be adopted by users. This is the version described
in this guide.\\
The C codes in this release use the same API as the implementation
in Release~1.2.0, but the code has been split into multiple files
so the build is more complicated. The MATLAB also retains the same
API, but class names have changed. The Java implementation is identical
to the implementation in Release~1.2.0.
\end{itemize}
The implementation in each language is separate and does not rely
on the others. The implementation includes MATLAB adapter classes
that allow invocation of the C and Java implementations from MATLAB.
This allows a single set of MATLABexample functions to invoke all
three implementations. 

The programming interfaces of all three implementations are similar.
They offer exactly the same functionality using the same abstractions,
and each employs good programming practices of the respected language.
For example, the MATLAB and Java implementations use overloading
(using the same method name more than once, with different argument
lists). Another example is a method that returns two values in the
MATLAB implementation, but only one in the others; the second value
is returned by a separate method or function in the Java and C implementations.
The only differences are ones that are unavoidable due to the constraints
of each programming language.

The MATLAB implementation does not rely on any MATLAB toolbox, only
on functionality that is part of the core product. The implementation
also works under GNU Octave (version 2.x seems not to work under
Octave due to the use of abstract classes; version 1.x does). The
C implementation relies on basic matrix and vector operations from
the BLAS~\cite{BLAS,BLAS3ALG} and on the QR and Cholesky factorizations
from LAPACK~\cite{LAPACK-UG}. The C implementation of the parallel-in-time
smoothers used the Threading Building Blocks (TBB) library to express
shared-memory parallelism. TBB is a C++ library and the code uses
a single C++ source file to expose the parallel primitives. The code
can also be compiled without TBB, but this generates a sequential
algorithm, not a multi-threaded (multi-core) one. The Java implementation
uses the Apache Commons Math library for both basic matrix-vector
operations and for the QR and Cholesky factorizations. The Cholesky
factorization is used only to factor covariance matrices that are
specified explicitly, as opposed to being specified by inverse factors
or triangular factors.

We first describe how the different implementations represent matrices,
vectors, and covariance matrices. Then we describe in detail the MATLAB
programming interface and implementation and then comment on the differences
between them and those of the other two implementations. The guide
ends with a discussion of the data structures that are used to represent
the step sequence and a presentation of a mechanism for measuring
the performance of the implementations.

The code has been tested with MATLAB R2021b (version 9.11) and R2024a,
and with GNU OCTAVE 7.1.0, both running under Windows 11. The code
has also been tested on Linux and on MacOS. Under Windows, MATLAB
was configured to use Microsoft Visual C/C++ 2019 to compile C (mex)
code. OCTAVE was configured to use mingw64 to compile C (mex) code.
The code also compiles as a standalone C program under both GCC
and Microsoft Visual C/C++ 2022, as well as under Java version 1.8
(also called version 8) and up.

The distribution archive contains a number of directories with scripts
that build libraries, programs, and this document. The scripts for
Windows are called \texttt{build.bat}. To run them, type \texttt{build}
on the Windows command prompt. The scripts for Linux and MacOS are
called \texttt{build.sh}. To run them, type \texttt{./build.sh} in
a shell (terminal window). The \texttt{build.sh} files must have permissions
that allows them to execute as scripts; unpacking the distribution
archive normally gives them this permission, but if you receive a
\texttt{permission denied} error message, give the file this premission
using the command \texttt{chmod +x build.sh} and try again.

The rest of this guild is organized as follows. Section~\ref{sec:matrices-and-vectors}
explains how vectors and matrices are represented in the three implementations.
Section~\ref{sec:covariance-matrices} explains how covariance matrices
are represented. Section~\ref{sec:matlab} presents the programming
interface of the MATLAB implementation and how to add it to MATLAB's
search path.

\section{\label{sec:matrices-and-vectors}The Representation of Vectors and
Matrices}

The MATLAB implementation uses native MATLAB matrices and vectors.
The Java implementation uses the types \texttt{RealMatrix} and \texttt{RealVector}
from the Apache Commons Math library~(both are interface types with
multiple implementations). 

The C implementation defines a type called \texttt{matrix\_t} to
represent matrices and vectors. The implementation defines functions
that implement basic operations of matrices and vectors of this type.
The type is implemented using a structure that contains a pointer
to an array of double-precision elements, which are stored columnwise
as in the BLAS and LAPACK, and integers that describe the number
of rows and columns in the matrix and the stride along rows (the so-called
leading dimension in the BLAS and LAPACK interfaces). To avoid name-space
pollution, in client code this type is called \texttt{kalman\_matrix\_t}.

State vectors are not always observable. This topic is explained in
Section ~3.2 in the companion article. This situation usually arises
when there are not enough observations to estimate the state. The
function calls and methods that return estimates of state vectors
and the covariance matrices of the estimates return in such cases
a vector of \texttt{NaN}s (not-a-number, a floating point value that
indicates that the value is not available) and a diagonal matrix whose
diagonal elements are \texttt{NaN}.

\section{\label{sec:covariance-matrices}The Representation of Covariance
Matrices}

Like all Kalman filters, UltimateKalman consumes covariance matrices
that describe the distribution of the error terms and produces covariance
matrices that describe the uncertainty in the state estimates $\estimate u_{i}$.
The input covariance matrices are not used explicitly; instead, the
inverse factor $W$ of a covariance matrix $C=(W^{T}W)^{-1}$ is multiplied,
not necessarily explicitly, by matrices or by a vector. 

Therefore, the programming interface of UltimateKalman expects input
covariance matrices to be represented as objects belonging to a type
with a method \texttt{weigh} that multiplies the factor $W$ by a
matrix $A$ or a vector $v$. In the MATLAB and Java implementations,
this type is called \texttt{CovarianceMatrix}. The constructors of
these classes accept many representations of a covariance matrix:
\begin{itemize}
\item An explicit covariance matrix $C$; the constructor computes an upper
triangular Cholesky factor $U$ of $C=U^{T}U$ and implements \texttt{X=C.weigh(A)}
by solving $UX=A$.
\item An inverse factor $W$ such that $W^{T}W=C^{-1}$; this factor is
stored and multiplied by the argument of \texttt{weigh}.
\item An inverse covariance matrix $C^{-1}$; the constructor computes its
Cholesky factorization and stores the lower-triangular factor as $W$.
\item A diagonal covariance matrix represented by a vector $w$ such that
$W=\text{diag}(w)$ (the elements of $w$ are inverses of standard
deviations).
\item A few other, less important, variants.
\end{itemize}
In the MATLAB implementation, the way that the argument to the constructor
represents $C$ is defined by a single-character argument (with values
\texttt{C}, \texttt{W}, \texttt{I}, and \texttt{w}, respectively).
In the Java implementation, \texttt{CovarianceMatrix} is an interface
with two implementing classes, \texttt{DiagonalCovarianceMatrix} and
\texttt{RealCovarianceMatrix}; the way that the numeric argument represents
$C$ is specified using \texttt{enum} constants defined in the implementation
classes:
\begin{lstlisting}
RealCovarianceMatrix.Representation.COVARIANCE_MATRIX
RealCovarianceMatrix.Representation.FACTOR
RealCovarianceMatrix.Representation.INVERSE_FACTOR
DiagonalCovarianceMatrix.Representation.COVARIANCE_MATRIX
DiagonalCovarianceMatrix.Representation.DIAGONAL_VARIANCES
DiagonalCovarianceMatrix.Representation.DIAGONAL_STANDARD_DEVIATIONS
DiagonalCovarianceMatrix.Representation.DIAGONAL_INVERSE_STANDARD_DEVIATIONS
\end{lstlisting}
The \texttt{RealCovarianceMatrix} class has a single constructor that
takes a \texttt{RealMatrix} and a representation constant. The \texttt{DiagonalCovarianceMatrix}
class several constructors that take either a \texttt{RealVector},
an array of \texttt{double} values, or a single \texttt{double} and
a dimension (the covariance matrix is then a scaled identity); all
also take as a second argument a representation constant.

Covariance input matrices are passed to the C implementation in a
similar manner, but without a class; each input covariance matrix
is represented using two arguments, a matrix and a single character
(\texttt{C}, \texttt{W}, \texttt{I}, or \texttt{w}) that defines how
the given matrix is related to $C$. 

The UltimateKalman algorithm always returns the covariance matrix
of $\estimate u_{i}$ as an upper triangular inverse factor $W$.
The other algorithms return an explicit covariance matrix $C$. The
MATLAB and Java implementations return covariance matrices as objects
of the \texttt{CovarianceMatrix} type (always with an inverse-factor
representation); the C implementation includes one function that returns
the type of the covariance matrix (a single character) and another
that returns the factor $W$ or the explicit matrix $C$.

\section{\label{sec:matlab}The MATLAB Programming Interface}

The MATLAB implementation resides in the \texttt{matlab} directory
of the distribution archive. To be able to use it, you must add this
directory to your MATLAB search path using MATLAB's \texttt{addpath}
command. 

The MATLAB implementation is object oriented and is implemented as
a collection of handle (reference) classes called \texttt{KalmanUltimate},
\texttt{KalmanConventional}, \texttt{KalmanOddevenSmoother}, \texttt{KalmanAssociativeSmoother},
and \texttt{KalmanSparse}. The first four classes implement, respectively,
the UltimateKalman algorithm~\cite{doi:10.1145/3699958,PaigeSaunders:1977:Kalman},
a conventional Kalman filter~\cite{Kalman:1960:KalmanFilter} and
RTS smoother~\cite{10.2514/3.3166}, the Gargir-Toledo parallel smoother~\cite{toappear:IPDPS2025},
and the parallel smoother by Särkkä and García-Fernández~\cite{10.1109/TAC.2020.2976316}.
The fifth implementation, \texttt{KalmanSparse}, uses an explicit
sparse QR factorizations to filter and smooth; it is inefficient,
especially when filtering, and is meant only for testing the correctness
of other implementations. It is particularly simple and therefore
it is particularly easy to ensure that it is correct. The interface
of all these classes is exactly the same. The constructor takes one
optional argument, a structure that specifies algorithmic options.

\begin{lstlisting}
kalman = KalmanUltimate(options)
\end{lstlisting}
or

\begin{lstlisting}
kalman = KalmanUltimate()
\end{lstlisting}

The (overloaded) methods that advance the filter through a sequence
of steps are \texttt{evolve} and \texttt{observe}. Each of them must
be called exactly once at each step, in this order. The \texttt{evolve}
method declares the dimension of the state of the next step and provides
all the known quantities of the evolution equation, 

\begin{lstlisting}
kalman.evolve(n_i, H_i, F_i, c_i, K_i)
\end{lstlisting}
where \texttt{n\_i} is an integer, the dimension of the state, \texttt{H\_i}
and \texttt{F\_i} are matrices, \texttt{c\_i} is a vector, and \texttt{K\_i}
is a \texttt{CovarianceMatrix} object. The number of rows in\texttt{
H\_i}, \texttt{F\_i}, and \texttt{c\_i} must be the same and must
be equal to the order of \texttt{K\_i}; this is the number $\ell_{i}$
of scalar evolution equations. The number of columns in \texttt{H\_i}
must be \texttt{n\_i} and the number of columns in \texttt{F\_i} must
be equal to the dimension of the previous step. A simplified overloaded
version defines \texttt{H\_i} internally as an $n_{i}$-by-$n_{i-1}$
identity matrix, possibly padded with zero columns

\begin{lstlisting}
kalman.evolve(n_i, F_i, c_i, K_i)
\end{lstlisting}
If $n_{i}>\ell_{i}$, this overloaded version adds the new parameters
to the end of the state vector.

If $n_{i}<\ell_{i}$, the first version must be used; this forces
the user to specify how parameters in $u_{i-1}$ are mapped to the
parameters in $u_{i}$. The \texttt{evolve} method must be called
even in the first step; this design decision was taken mostly to keep
the implementation of all the steps in client code uniform. In the
first step, there is no evolution equation, so the user can pass empty
matrices to the method, or call another simplified overloaded version:

\begin{lstlisting}
kalman.evolve(n_i)
\end{lstlisting}

The \texttt{observe} method comes in two overloaded versions. One
of them must be called to complete the definition of a step. The first
version describes the observation equation and the second tells UltimateKalman
that there are no observations of this step.

\begin{lstlisting}
kalman.observe(G_i, o_i, C_i)
kalman.observe()
\end{lstlisting}

Steps are named using zero-based integer indices; the first step that
is defined is step~$i=0$, the next is step~$1$, and so on. The
\texttt{estimate} methods return the estimate of the state at step
\texttt{i} and optionally the covariance matrix of that estimate,
or the estimate and covariance of the latest step that is still in
memory (normally the last step that was observed):

\begin{lstlisting}
[estimate, covariance] = kalman.estimate(i)
[estimate, covariance] = kalman.estimate()
\end{lstlisting}
If a step is not observable, \texttt{estimate} returns a vector of
$n_{i}$ \texttt{NaN}s (not-a-number, an IEEE-754 floating point representation
of an unknown quantity). 

The \texttt{forget} methods delete from memory the representation
of all the steps up to and including $i$, or all the steps except
for the latest one that is still in memory.

\begin{lstlisting}
kalman.forget(i)
kalman.forget()
\end{lstlisting}
The \texttt{rollback} methods return the filter to its state just
after the invocation of \texttt{evolve} in step \texttt{i}, or just
after the invocation of \texttt{evolve} in the latest step still in
memory.

\begin{lstlisting}
kalman.rollback(i)
kalman.rollback()
\end{lstlisting}
The methods \texttt{earliest} and \texttt{latest} are queries that
take no arguments and return the indices of the earliest and latest
steps that are still in memory.

The \texttt{smooth} method, which also takes no arguments, computes
the smoothed estimates of all the states still in memory, along with
their covariance matrices. After this method is called, \texttt{estimate}
returns the smoothed estimates. A single step can be smoothed many
times; each smoothed estimate will use the information from all past
steps and the information from future steps that are in memory when
\texttt{smooth} is called.

To use the C or Java implementations from within MATLAB, create
an object of one of the adapter classes:
\begin{lstlisting}
kalman = KalmanNative(options)
kalman = KalmanJava(options)
\end{lstlisting}
These adapter classes have exactly the same interface as the MATLAB
implementations (including the fact that the \texttt{options} argument
is optional).

To use the C implementation, you will first need to compile the C
code into a MATLAB-callable dynamically-linked library that MATLAB
uses through an interface called the \texttt{mex} interface. To perform
this step, run the \texttt{UltimateKalman\_build\_\LyXZeroWidthSpace mex.m}
script in the \texttt{matlab} directory. To use the Java implementation,
build the Java library using the instructions in Section~\ref{sec:java},
and add both the resulting library (a \texttt{jar} file) and the library
containing the Apache Commons Math library to MATLAB's Java search
path using MATLAB \texttt{javaaddpath} command. The function \texttt{replication.m}
in the \texttt{examples} sub-directory adds these libraries to the
path and can serve as an example.

\subsection{Options}

The MATLAB implementations interpret a number of options, specified
as fields of the \texttt{options} structure. Most of the implementations
process only a subset of the options (some process none), as shown
in Table~\ref{tab:matlab-options}.
\begin{table}
\begin{centering}
\begin{tabular}{ll>{\raggedright}p{0.2\textwidth}ccccccc}
field name & type & values (default is 1st) & \begin{turn}{90}
\texttt{KalmanUltimate}
\end{turn} & \begin{turn}{90}
\texttt{KalmanConventional}
\end{turn} & \begin{turn}{90}
\texttt{KalmanOddevenSmoother}
\end{turn} & \begin{turn}{90}
\texttt{KalmanAssociativeSmoother}
\end{turn} & \begin{turn}{90}
\texttt{KalmanSparse}
\end{turn} & \begin{turn}{90}
\texttt{KalmanNative}
\end{turn} & \begin{turn}{90}
\texttt{KalmanJava}
\end{turn}\tabularnewline
\hline 
\texttt{estimateCovariance} & boolean & \texttt{true, false} & $\checkmark$ &  & $\checkmark$ &  &  &  & \tabularnewline
\cline{1-6} \cline{2-6} \cline{3-6} \cline{4-6} \cline{5-6} \cline{6-6} 
\texttt{covarianceEstimates} & string & \texttt{'PaigeSaunders', 'SelInv'} & $\checkmark$ &  &  &  &  &  & \tabularnewline
\cline{1-6} \cline{2-6} \cline{3-6} \cline{4-6} \cline{5-6} \cline{6-6} 
\texttt{smoothOnly} & boolean & \texttt{false, true} &  &  &  &  & $\checkmark$ &  & \tabularnewline
\cline{1-6} \cline{2-6} \cline{3-6} \cline{4-6} \cline{5-6} \cline{6-6} 
\texttt{algorithm} & string & \texttt{'Ultimate', 'Conventional', 'Oddeven', 'Associative'} &  &  &  &  &  & $\checkmark$ & \tabularnewline
\end{tabular}
\par\end{centering}
\caption{\label{tab:matlab-options}Options that affect the behavior of the
MATLAB implementations.}

\end{table}

The meaning of the options is as follows:
\begin{itemize}
\item The \texttt{estimateCovariance} field tells some of the algorithms
that the covariance matrices of the state estimates are not required.
Not computing them saves time in these algorithms. This is particularly
useful in smoothers that are used as a building block of a non-linear
smoother~\cite{10.1109/ICASSP40776.2020.9054686}.
\item The \texttt{covarianceEstimates} field specifies which method to use
to compute the covariance matrices of the state estimates, when more
than one method is available. 
\item The \texttt{smoothOnly} field tells an implementation that filtered
estimates are not required, only smoothed estimates. This can save
time in some implementations (most notably, the \texttt{KalmanSparse}
one).
\item The \texttt{algorithm} field tells the interface to the C implementations
which algorithm to use.
\end{itemize}

\subsection{Implementation Details}

All the implementations extend an abstract class called \texttt{KalmanBase},
which implements most of the methods that handle modifications of
the sequence of steps (forgetting, rolling back, etc.). The \texttt{KalmanOddevenSmoother}
and the \texttt{KalmanAssociative\-Smoother} implementations do not
return filtered estimated, only smoothed estimates. They both extend
a second abstract class, \texttt{KalmanExplicitRepresentation}, in
which the \texttt{evolve} and \texttt{observe} methods simply record
the equations for later processing by the smoother. Both of these
implementations are sequential but they exhibit the parallel algorithms
in a clear way that is hopefully easy to implement in parallel programming
environments.

\section{\label{sec:java}The Java Programming Interface}

The programming interface to the Java implementation is nearly identical.
The implementing class is \texttt{sivantoledo\LyXZeroWidthSpace .kalman\LyXZeroWidthSpace .UltimateKalman}.
It also uses overloaded methods to express default values. It differs
from the MATLAB interface only in that the \texttt{estimate} methods
return only one value, the state estimate. To obtain the matching
covariance matrix, client code must call a separate method, \texttt{covariance}.

\subsection{Building and Running the Java Code}

The Java implementation resides under the \texttt{java} directory
of the distribution archive. The sources are in the \texttt{src} subdirectory.
To use it, you first need to compile the source code and to assemble
the compiled code into a library (a \texttt{jar} file). The scripts
\texttt{build.bat} and \texttt{build.sh}, both in the \texttt{java}
directory, perform these steps under Windows (\texttt{build.bat})
and Linux and MacOS (\texttt{build.sh}). To run the scripts, your
computer must have a Java development kit (JDK) installed. We used
successfully releases of OpenJDK on both Windows and Linux. The code
is compiled so that the library can be used with any version of Java
starting with version~8 (sometimes also referred to as 1.8).

The build scripts also compile and run an example program, \texttt{Rotation.java}.
It performs the same computations as the MATLAB example function
\texttt{rotation.m} when executed with arguments \texttt{rotation(UltimateKalman,5,2)}.
This program serves as an example that shows how to write Java code
that calls UltimateKalman.

\section{\label{sec:c-lang}The C programming interface}

In the C interface, defined in \texttt{kalman.h}, the filter is represented
by a pointer to a structure of the \texttt{kalman\_t} type; to client
code, this structure is opaque (there is no need to directly access
its fields). The filter is constructed by a call to \texttt{kalman\_create}
or \texttt{kalman\_create\_options}, which returns a pointer to \texttt{kalman\_t}.
The interface to the parallel smoothers is a little different and
will be explained later.

In general, the memory management principle of the interface (and
the internal implementation) is that client code is responsible for
freeing memory that was allocated by a call to any function whose
name includes the word \texttt{create}. Therefore, when client code
no longer needs a filter, it must call \texttt{kalman\_free} and pass
the pointer as an argument. 

The functionality of the filter is exposed through functions that
correspond to methods in the MATLAB and Java implementations. These
functions expect a pointer to \texttt{kalman\_t} as their first argument.
The functions are not overloaded because C does not support overloading.
Missing matrices and vectors (e.g., to \texttt{evolve} and \texttt{observe})
are represented by a \texttt{NULL} pointer and default step numbers
(to \texttt{forget} , \texttt{estimate}, and so on) by $-1$. Here
is the declaration the functions. 
\begin{lstlisting}
kalman_t*        kalman_create         ();
kalman_t*        kalman_create_options (kalman_options_t options);
void             kalman_free           (kalman_t* kalman);

void             kalman_evolve    (kalman_t* kalman, int32_t n_i,                                                      
                                   kalman_matrix_t* H_i, 
                                   kalman_matrix_t* F_i, kalman_matrix_t* c_i,
                                   kalman_matrix_t* K_i, char K_i_type);
void             kalman_observe   (kalman_t* kalman,                                                               
                                   kalman_matrix_t* G_i, kalman_matrix_t* o_i,
                                   kalman_matrix_t* C_i, char C_i_type);

int64_t          kalman_earliest  (kalman_t* kalman);
int64_t          kalman_latest    (kalman_t* kalman);
void             kalman_forget    (kalman_t* kalman, int64_t i);
void             kalman_rollback  (kalman_t* kalman, int64_t i);

void             kalman_smooth    (kalman_t* kalman);

kalman_matrix_t* kalman_estimate        (kalman_t* kalman, int64_t i);
kalman_matrix_t* kalman_covariance      (kalman_t* kalman, int64_t i);
char             kalman_covariance_type (kalman_t* kalman, int64_t i);

kalman_matrix_t* kalman_perftest        (kalman_t* kalman, ...); // see later
\end{lstlisting}
Note that input covariance matrices are represented by a \texttt{kalman\_matrix\_t}
and a representation code (a single character). The output of \texttt{kalman\_covariance}
is a matrix $W$ such that $C=(W^{T}W)^{-1}$ or $C$ itself, where
$C$ is the covariance matrix of the output of \texttt{kalman\_estimate}
on the same step. The function \texttt{kalman\_covariance\_type} specifies
whether $C$ is represented by itself or by its inverse factor $W$.

A small set of helper functions allows client code to construct input
matrices in the required format, to set their elements, and to read
and use matrices returned by UltimateKalman. Here are the declarations
of some of them (the full set is declared in \texttt{matrix\_ops.h},
included automatically by \texttt{kalman.h}).
\begin{lstlisting}
kalman_matrix_t* matrix_create(int32_t rows, int32_t cols);
void             matrix_free(kalman_matrix_t* A);

void   matrix_set(kalman_matrix_t* A, int32_t i, int32_t j, double v);
double matrix_get(kalman_matrix_t* A, int32_t i, int32_t j);

int32_t matrix_rows(kalman_matrix_t* A);
int32_t matrix_cols(kalman_matrix_t* A);
...
\end{lstlisting}

Client code is responsible for freeing matrices returned by \texttt{kalman\_estimate}
and \texttt{kal\-man\_\-covariance} by calling \texttt{matrix\_free}
when they are no longer needed.

\subsection{Options}

The C takes an integer-type options argument that specifies which
algorithm to use, as well as parameters for these algorithms (currently
only one parameter is supported). Different options are ORed together.
The option values defined in \texttt{kalman.h} are:
\begin{itemize}
\item \texttt{KALMAN\_ALGORITHM\_ULTIMATE}, \texttt{KALMAN\_ALGORITHM\_CONVENTIONAL},
\texttt{KALMAN\_\-ALGORITHM\_\-ODDEVEN}, and \texttt{KALMAN\_ALGORITHM\_ASSOCIATIVE},
which specify which algorithm to use. Specify exactly one of these
bit values.
\item \texttt{KALMAN\_NO\_COVARIANCE}, which tells the UltimateKalman and
the Oddeven algorithms not to compute the covariance matrices of the
estimates.
\end{itemize}

\subsection{Direct Invocation of the Parallel Smoothers}

The two parallel smoothers can be invoked for testing by first supplying
the evolution and observation equations using a sequence of calls
to \texttt{kalman\_evolve} and \texttt{kalman\_observe} and then calling
\texttt{kalman\_smooth}, but this normally not appropriate for high-performance
parallel software, because the construction of the input to the smoother
is completely sequential.

Instead, the parallel smoothers should be invoked directly on an array
of equations,
\begin{lstlisting}
void kalman_smooth_oddeven(kalman_options_t         options, 
                          kalman_step_equations_t** equations, 
                          kalman_step_index_t       k); 
void kalman_smoother_associative(...); // same arguments
\end{lstlisting}
The first argument contains options, as in the call to \texttt{kalman\_create\_options}.
The second, \texttt{equations}, is an array of \texttt{k} pointers
to structures that each define an evolution equation and an observation
equation, defined in \texttt{kalman.h}:
\begin{lstlisting}
typedef struct kalman_step_equations_st {
  kalman_step_index_t step; // logical step number, start from 0
  int32_t dimension;

  kalman_matrix_t* H;
  kalman_matrix_t* F;
  kalman_matrix_t* c;
  kalman_matrix_t* K;
  char             K_type;

  kalman_matrix_t* G;
  kalman_matrix_t* o;
  kalman_matrix_t* C;
  char             C_type;

  kalman_matrix_t* state;
  kalman_matrix_t* covariance;
  char             covariance_type;
} kalman_step_equations_t;
\end{lstlisting}

Parallel codes should normally allocate two arrays of length \texttt{k},
an arrays of structures and an array of pointers to these structures,
and ideally fill their elements using a multi-threaded code. The array
of pointers should be filled with the addresses of the elements in
the array of structures. Each structure should be filled with the
matrices that define the evolution and observation equations of one
step; the semantics are the same as in calls to \texttt{kalman\_evolve}
and \texttt{kalman\_observe}. The \texttt{dimension} field specifies
the dimension of the state vector, and the \texttt{step} field is
a sequence number that should start from zero.

When the call to the smoother returns, the \texttt{state} fields contain
the smoothed state estimates and the \texttt{covariance} and \texttt{covariance\_type}
fields contain the covariance of the state, unless the \texttt{KALMAN\_NO\_COVARIANCE}
bit was set in \texttt{options}.

The parallel-programming environment that the library uses, TBB, allows
codes to control the number of operating-system threads that are used
in a given computation, and a parameter called the block size. The
first parameter controls how many physical cores are used. The second
controls the overhead of parallelism: in parallel for loops and similar
constructs, the environment performs blocks of iterations sequentially
to reduce overhead. Our experiments indicate that a value of 16 is
a good for the block size. Callers should specify these two value,
using the following two functions defined in \texttt{parallel.h},
before calling a parallel smoother:
\begin{lstlisting}
void parallel_set_thread_limit(int p); 
void parallel_set_blocksize   (int b); 
\end{lstlisting}


\subsection{Building and Running the C Codes Outside MATLAB}

To help you integrate UltimateKalman into your own native code in
C or C++, the software includes several example programs that call
UltimateKalman and Windows and Linux/MacOS scripts that build executable
versions of the three programs.  You should be able to use these
scripts as examples of how to compile and link UltimateKalman. 

These C programs, the C implementation of UltimateKalman, and the
build scripts are all under the \texttt{c} directory of the distribution
archive.

\subsubsection*{Building Under Windows}

The Windows build script \texttt{build.bat} uses the BLAS and LAPACK
libraries from MKL~\cite{MKL}. Both MKL and TBB are installed as
part of the oneAPI Base Kit, a set of free development tools from
Intel. The kit includes additional components but for UltimateKalman,
only thse components are required. We tested the library with the
2024.1 and 2025.0 versions of oneAPI, as well as some older ones.
Upgrading oneMKL should not require any change in the build script.

The script also uses Microsoft's C/C++ compiler (\texttt{cl}) from
the free Visual Studio Community Edition. We tested the library with
versions 2019 and 2022~\cite{VSCommunity}. The build script is currently
set up to use the 2022 version. If you installed a different version,
you will need to update the path to the script that sets up the development
tool in \texttt{build.bat}.

Assuming that these prerequisites have been installed, you can build
the example programs by executing \texttt{build.bat}. The script will
build the programs and it will tell you to execute another script
so that Windows can find the MKL libraries at run time.
\begin{lstlisting}
C:\Users\stoledo\github\ultimate-kalman\c>build.bat
**********************************************************************
** Visual Studio 2022 Developer Command Prompt v17.9.2
** Copyright (c) 2022 Microsoft Corporation
**********************************************************************
[vcvarsall.bat] Environment initialized for: 'x64'
:: initializing oneAPI environment...
   Initializing Visual Studio command-line environment...
   Visual Studio version 17.9.2 environment configured.
   "C:\Program Files\Microsoft Visual Studio\2022\Community\"
   Visual Studio command-line environment initialized for: 'x64'
:  advisor -- latest
:  compiler -- latest
:  dev-utilities -- latest
:  ipp -- latest
:  mkl -- latest
:  ocloc -- latest
:  pti -- latest
:  tbb -- latest
:  umf -- latest
:: oneAPI environment initialized ::
kalman_ultimate.c
kalman_conventional.c
kalman_oddeven_smoother.c
kalman_associative_smoother.c
...
generated test programs
To run the generated binaries, invoke
  "C:\Program Files (x86)\Intel\oneAPI\setvars.bat" intel64
on the command line, to ensure that Windows can find the required DLLs.
\end{lstlisting}
You can see in the output the setup of Visual Studio, then the setup
of oneAPI, and then the compilation of the source files. Once you
run \texttt{setvars.bat} so that Windows can find the libraries, you
can run the test programs. The output of \texttt{blastest.exe}, the
simplest example program, should look like this:
\begin{lstlisting}
C:\Users\stoledo\github\ultimate-kalman\c>blastest
BLAS test starting
A = matrix_print 2 3
1 2 3
4 5 6
B = matrix_print 3 2
7 8
9 10
12 13
C = matrix_print 2 2
125 137
293 323
Result should be:
  125  137
  293  323
BLAS test done
\end{lstlisting}


\subsubsection*{Building Under Linux}

Under Linux, \texttt{build.sh} is set up to use high-performance BLAS
and LAPACK libraries, normally libraries provides by the CPU vendor:
\begin{itemize}
\item On Intel CPUs, the script uses both MKL~\cite{MKL} and TBB from
Intel's free oneAPI base development kit. Install using the instructions
on Intel's web site. The script contains a variable that must be set
up to the path where oneAPI was installed. This path is normally \texttt{/opt/intel/oneAPI},
but if you installed it elsewhere, you will need to update this path
in the script.
\item On AMD CPUs, the script uses the BLAS and LAPACK from the AMD performance
libraries, which you can download freely from the AMD web site. Again
there is a path that might need to be modified. You will also need
to install TBB separately. On Ubuntu Linux, you can install it using
the command \texttt{sudo apt install libtbb-dev}.
\item On ARM processors, the script uses the BLAS and LAPACK from the ARM
performance libraries, which you can download freely from the ARM
web site. Again there is a path that might need to be modified. You
will also need to install TBB separately. On Ubuntu Linux, you can
install it using the command \texttt{sudo apt install libtbb-dev}.
\end{itemize}
In principle, for testing you can also use the BLAS and LAPACK libraries
that the Linux distribution provides. On Ubuntu, you can install them
using the commands
\begin{lstlisting}
sudo apt install libblas-dev
sudo apt install liblapack-dev
\end{lstlisting}
For small state dimensions these libraries might perform similarly
to the vendor's high-performance libraries, but for large dimensions
the CPU-vendor's libraries typically perform better.

\subsubsection*{Building Under MacOS}

The \texttt{build.sh} script assumes that on MacOS, XCode is installed
(Apple's development environment), which provides a C/C++ compiler.
The script also assumes that TBB has been installed using Homebrew,
a popular package manager for MacOS. The compiler is invoked in the
script using the \texttt{gcc} command, but the actual compiler is
not the GNU C compiler (\texttt{gcc}) but \texttt{clang}. The BLAS
and LAPACK libraries that we use in the MacOS build are from Apple's
Accelerate framework, a collection of high-performance libraries.

To install TBB using Homebrew, first install Homebrew (if not already
installed) and then issue the commands \texttt{brew update} and \texttt{brew
install tbb}. To inspect the installed version, use the command \texttt{brew
info tbb}.

\subsubsection*{Running Example Programs}

The build script builds two binaries from each of the four example
source files, a sequential binary and a multi-threaded binary compiled
using TBB. The multi-threaded binaries have names that end with \texttt{\_par},
such as \texttt{performance\_par}. The examples are:
\begin{itemize}
\item \texttt{blastest}, a simple program that only tests that UltimateKalman
is able to call BLAS and LAPACK functions correctly. If this program
fails to compile, to link, or to run, there is a problem with the
BLAS and LAPACK libraries. 
\item \texttt{rotation} performs the same computation that the expression
\texttt{rotation(\-KalmanUltimate,5,2)} performs in the MATLAB version
and should output the same numerical results, just like \texttt{Rotation.java}.
This example shows how to use UltimateKalman to filter and smooth,
and it shows how the APIs in the three languages are related to each
other.\\
The program accepts up to 3 parameters, specified on the command-line
as key-value pairs: \texttt{nthreads}, \texttt{blocksize}, and \texttt{algorithm}.
The first takes an integer value (e.g., \texttt{nthreads=4}) and limits
TBB to use this number of operating-system threads, and hence this
number of cores. If you omit this parameter, TBB will use all the
cores available. The second controls TBB's block size, the number
of elements that are processed sequentially in parallel loops. The
default value is 16. The third tells the program which algorithm to
use, with possible values \texttt{Ultimate}, \texttt{Conventional},
\texttt{Oddeven}, and \texttt{Associative}.
\item \texttt{performance} implements a Kalman smoother on problems with
$n_{i}=m_{i}=n$ and $k$ steps. The matrices $F_{i}$ and $G_{i}$
are orthonormal and $H_{i}=I$. These choices avoid the risk of numerical
problems, including overflows or underflows. The program measures
and reports the running time it takes to set up the problem and to
perform filtering (unless the algorithm selected is a smoother), and
the time it takes to perform smoothing. You specify $n$ and $k$
on the command line. Setting $n=6$ or $n=48$ results in exactly
the same numerical input as the MATLAB version (with seed 1 for the
random-number generator).\\
The program accepts all the parameters that \texttt{rotation} accepts,
but also \texttt{n}, \texttt{k}, and \texttt{nocov}. The last is a
binary parameter that tells the program, if set, to not compute the
covariance matrices of the state estimates. 
\end{itemize}

\subsubsection*{Modifying Build Parameters}

Certain aspects of UltimateKalman can be modified by defining preprocessor
macros, listed in Table~\ref{tab:C-pre-variables}:
\begin{itemize}
\item The BLAS and LAPACK were originally Fortran libraries and different
implementations of them must be called from C in different ways. Several
proprocessor macros select the calling convention. The build scripts
set these up for several popular libraries. 
\item Step numbers (indices of states) can be represented by 32-bit or 64-bit
numbers. Three preprocessor macros, which should be set consistently,
control this choice: \texttt{KALMAN\_STEP\_INDEX\_TYPE\_INT32}, \texttt{FARRAY\_INDEX\_TYPE\_INT32},
and \texttt{PARALLEL\_INDEX\_TYPE\_INT32} (to use 64-bit integers,
replace \texttt{INT32} by \texttt{INT64}; it is also possible to use
unsigned integer typed, but we advise against this). 
\item Two preprocessor macros listed at the bottom of the table control
debug outputs. They should normally not be set.
\end{itemize}
\begin{table}
\begin{centering}
\begin{tabular}{ll>{\raggedright}p{0.4\textwidth}}
\hline 
variable name & requires a value? & explanation\tabularnewline
\hline 
\texttt{BUILD\_MKL} & no & specifies that MKL is used; sets all the other BLAS and LAPACK
variables (so you do not need to)\tabularnewline
\texttt{BUILD\_BLAS\_INT} & yes & C data type that specifies a row or column index or a dimension of
a matrix; usually either \texttt{int32\_t} or \texttt{int64\_t}\tabularnewline
\texttt{HAS\_BLAS\_H} & no & specifies that the \texttt{blas.h} header is available\tabularnewline
\texttt{HAS\_LAPACK\_H} & no & specifies that the \texttt{lapack.h} header is available\tabularnewline
\texttt{BUILD\_BLAS\_UNDERSCORE} & no & add an underscore to names of BLAS and LAPACK functions\tabularnewline
\texttt{BUILD\_BLAS\_STRLEN\_END} & no & add string-length arguments to calls to the BLAS and LAPACK\tabularnewline
\hline 
\texttt{KALMAN\_STEP\_INDEX\_TYPE\_INT32} & exactly one & Specifies 32-bit step numbers\tabularnewline
\texttt{KALMAN\_STEP\_INDEX\_TYPE\_INT64} & exactly one & Speficies 64-bit step numbers\tabularnewline
\texttt{FARRAY\_INDEX\_TYPE\_INT32/64} & yes & Must be consistent with \texttt{KALMAN\_STEP\_INDEX\_TYPE}\tabularnewline
\texttt{PARALLEL\_INDEX\_TYPE\_INT32/64} & yes & Must be consistent with \texttt{KALMAN\_STEP\_INDEX\_TYPE}\tabularnewline
\hline 
\texttt{BUILD\_DEBUG\_PRINTOUTS} & no & generates run-time debug printouts\tabularnewline
\texttt{NDEBUG} & no & suppresses run-time assertion checking\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:C-pre-variables}Preprocessor variables that control how
UltimateKalman calls the BLAS and LAPACK, as well as several other
aspects of its behavior.}
\end{table}


\section{\label{sec:testing-and-examples}MATLAB Tests and Code Examples }

The software distribution of UltimateKalman includes several MATLAB
example and testing functions, stored in the \texttt{examples} sub-directory.
They demonstrate how to use the library and help test it. The directory
also contains a function that builds the MATLAB-callable version of
the C libray. All of these MATLAB files are documented with comments
that can be accessed through the MATLAB \texttt{help} command (e.g.,
\texttt{help rotation}). Not all the examples work with all the Kalman
filtering and smoothing algorithms. 

The script \texttt{UltimateKalman\_build\_mex} builds the MATLAB-callable
version of the C (native) library. The produced native library is
a file in a format called a mex file. 

The examples take as input a Kalman filter factory function, which
allows them to instantiate one or more instances of the same algorithm.
You construct these factory functions by calling \texttt{kalmanFactory}
function, which accepts the name of a class and optionally a structure
with options for its constructor. For example:
\begin{lstlisting}
ultimateFactory = kalmanFactory('UltimateKalman');
nativeFactory   = kalmanFactory('KalmanNative',struct('algorithm','Conventional'));
rotation( ultimateFactory, 5, 2 );
rotation( nativeFactory,   5, 2 );
\end{lstlisting}

The examples are:
\begin{itemize}
\item \texttt{rotation.m}, modeling a rotating point in the plane (this
example is also implemented in C and Java, as described above).
This example should work with all the Kalman algorithms, but in implementations
that only smooth (do not filter), it will obviously not produce filtered
stated.
\item \texttt{constant.m}, modeling an fixed scalar or a scalar that increases
linearly with time, and with observation covariance matrices that
are identical in all steps except perhaps for one exceptional step.
\item \texttt{add\_remove.m}, demonstrating how to use $H_{i}$ to add or
remove parameters from a dynamic system. This example does not work
with the conventional Kalman filter-smoother and does not work with
the associative smoother, which both do not support non-identity $H_{i}$s.
\item \texttt{projectile.m}, implementing the model and filter of a projectile
described by Humpherys et al.~\cite{AFreshLook2012}.
\item \texttt{clock\_offsets.m}, implementing clock-offset estimation in
a distributed sensor system. This example demonstrates how to handle
parameters that appear only in one step and it too depends on non-identity
$H_{i}$s.
\end{itemize}
The mathematical details of these models are described in the article
that describes UltimateKalman. 

The file \texttt{performance.m} contains a function that tests and
plots the performance of Kalman filters and smoothers. Its behavior
is described in the next section.

The file \texttt{compare.m} contains a function that runs two Kalman
filters and/or smoothers on the same synthetic problem and compares
their result. Its main function is to test the correctness of one
algorithm or implementation to another known to be correct. It does
not currently provide a complete coverage of all the code in all the
implementation, but it is still very effective in detecting bugs.
The gold standard for the known-correct filter/smoother is the \texttt{KalmanSparse}
implementation, which is the simplest so its correctness is easiest
to validate analytically, but it is slow, especially when filtering.
A reasonable strategy is to use it to validate the correctness of
another filter/smoother once, say \texttt{KalmanUltimate}, and then
to use this filter/smoother to test the other implementations and
algorithms. The main current limitation of this function is that it
always uses $H_{i}=I$.

The script \texttt{replication.m}, also in the \texttt{examples} sub-directory,
runs all of these examples and optionally generates the figures shown
in Section~5 of the article on UltimateKalman~\cite{doi:10.1145/3699958}.
Comparing the generated figures to those in the article provides visual
evidence that the code runs correctly. The script can run not only
the MATLAB implementation, but also the C and Java implementations.
This script assumes that the corresponding libraries have already
been built. The version of the C library that MATLAB uses is built
using the \texttt{UltimateKalman\_build\_\LyXZeroWidthSpace mex.m}
script, as explained in Section~\ref{sec:c-lang} above. The Java
version should be built outside MATLAB, as explained in Section~\ref{sec:java}.
To run the script to generate the graphs inn~\cite{doi:10.1145/3699958},
run it with no arguments or with the arguments
\begin{lstlisting}
replication(kalmanFactory('KalmanUltimate'), ...
    false, ...
    { kalmanFactory('KalmanUltimate'), ...
      kalmanFactory('KalmanJava'),     ...
      kalmanFactory('KalmanNative')    ...
    });
\end{lstlisting}
To test another implementation, run it with another factory as the
first argument, as in 
\begin{lstlisting}
replication(kalmanFactory('KalmanNative',struct('algorithm','Ultimate')));
\end{lstlisting}


\section{\label{sec:performance-testing}Support for Performance Testing}

All the implementations include a method, called \texttt{perftest},
designed for testing the performance of the filter. This method accepts
as arguments all the matrices and vectors that are part of the evolution
and observation equations, a step count, and an integer $d$ that
tells the method how often to take a wall-clock timestamp. The method
assumes that the filter has not been used yet and executes the filter
for the given number of steps. In each step, the state is evolved
and observed, the state estimate is requested, and the previous step
(if there was one) is forgotten. The same fixed matrices and vectors
are used in all steps.

This method allows us to measure the performance of all the implementations
without the overheads associated with calling C or Java from MATLAB.
That is, the C functions are called in a loop from C, the Java
methods are called from Java, and the MATLAB methods from MATLAB.

The method takes a timestamp every $d$ steps and returns a vector
with the average wall-clock running time per step in each nonoverlapping
group of $d$ steps.

The function \texttt{performance} uses the \texttt{perftest} method
to measure and plot the performance of one or more filters/smoothers.
You invoke it on a cell array of factories and provide several parameters:
a seed for the random-number generator, an array of state dimensions
to test, the number of state in each test, and the number of time
steps between timestamps. Graphs produced by this function are presented
in~\cite{doi:10.1145/3699958}.

\section{\label{sec:step-data-structure}Data Structures for the Step Sequence}

The information in this section helps to understand the implementations,
but is essentially irrelevant to users of UltimateKalman.

The Java implementation uses an \texttt{ArrayList} data structure
to represent the sequence of steps that have not been forgotten or
rolled back, along with an integer that specifies the step number
of the first step in the \texttt{ArrayList}. The data structure allows
UltimateKalman to add steps, to trim the sequence from both sides,
and to access a particular step, all in constant time or amortized
constant time.

The C implementation uses a specialized data structure with similar
capabilities. This data structure, called in the code \texttt{farray\_t},
is part of UltimateKalman. The sequence is stored in an array. When
necessary, the size of the array is doubled. The active part of the
array is not necessarily in the beginning, if steps have been forgotten.
When a step is added and there is no room at the end of the physical
array, then either the array is reallocated at double its current
size, or the active part is shifted to the beginning. This allows
the data structure to support appending, trimming from both sides,
and direct access to a step with a given index, again in constant
or amortized constant time.

The MATLAB implementation stores the steps in a cell array. The implementation
is simple, but not as efficient as the data structure that is used
by the C version.

\bibliographystyle{plainurl}
\bibliography{kalman}

\end{document}
